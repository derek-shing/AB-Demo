{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS2_433_Keras_Lecture.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derek-shing/AB-Demo/blob/master/LS_DS2_433_Keras_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJzTIkYAsLxw",
        "colab_type": "text"
      },
      "source": [
        "# Lets Use Libraries!\n",
        "\n",
        "The objective of the last two days has been to familiarize you with the fundamentals of neural networks: terminology, structure of networks, forward propagation, error/cost functions, backpropagation, epochs, and gradient descent. We have tried to reinforce these topics by requiring to you code some of the simplest neural networks by hand including Perceptrons (single node neural networks) and Multi-Layer Perceptrons also known as Feed-Forward Neural Networks. Continuing to do things by hand would not be the best use of our limited time. You're ready to graduate from doing things by hand and start using some powerful libraries to build cutting-edge predictive models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYCXc5KdkPE",
        "colab_type": "text"
      },
      "source": [
        "# Keras\n",
        "\n",
        "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
        "\n",
        "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "Runs seamlessly on CPU and GPU.\" \n",
        "\n",
        "## Installation\n",
        "\n",
        "The Keras API is particularly straightforward and it already comes pre-installed on Google Colab! \n",
        "\n",
        "<img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/pip-freeze-keras.png\" width=\"300\">\n",
        "\n",
        "If you're not on Google Colab you'll need to install one of the \"backend\" engines that Keras runs on top of. I recommend Tensorflow:\n",
        "\n",
        "> `pip install tensorflow`\n",
        "\n",
        "Google Colab does not have the latest Tensorflow 2.0 installation, so you'll need to upgrade to that if you want to experiment with it. However Tensorflow 2.0 was just released last month and is still in \"alpha\" so if you **really** want to use the latest and greatest be prepared for odd bugs that you don't have control over every once in a while. <https://www.tensorflow.org/install/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZYZZ3BpTEa3",
        "colab_type": "code",
        "outputId": "946ea578-3415-4339-ea04-f8ae6b253b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEncs0SOsFMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use pip freeze to see what packages/libraries your notebook has access to\n",
        "# !pip freeze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxgUUpIKn54a",
        "colab_type": "text"
      },
      "source": [
        "## Our First Keras Model - Perceptron, Batch epochs\n",
        "\n",
        "1) Load Data\n",
        "\n",
        "2) Define Model\n",
        "\n",
        "3) Compile Model\n",
        "\n",
        "4) Fit Model\n",
        "\n",
        "5) Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5D67XwqVAf",
        "colab_type": "text"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "Our life is going to be easier if our data is already cleaned up and numeric, so lets use this dataset from Jason Brownlee that is already numeric and has no column headers so we'll need to slice off the last column of data to act as our y values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFAWjnW6Te4X",
        "colab_type": "code",
        "outputId": "d7b38fb9-5d10-477d-d6fc-12c34959c3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-08 16:36:48--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23278 (23K) [text/plain]\n",
            "Saving to: ‘pima-indians-diabetes.data.csv’\n",
            "\n",
            "pima-indians-diabet 100%[===================>]  22.73K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-05-08 16:36:54 (1.92 MB/s) - ‘pima-indians-diabetes.data.csv’ saved [23278/23278]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKuofD3Pogil",
        "colab_type": "code",
        "outputId": "2d31b097-1c1f-4633-afa3-3219a7e77263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "import numpy\n",
        "# Load Pima Indians Dataset\n",
        "dataset = numpy.loadtxt('pima-indians-diabetes.data.csv', delimiter=',')\n",
        "# Split into X and Y variables\n",
        "X = dataset[:,0:8]\n",
        "print(X.shape)\n",
        "print(X)\n",
        "Y = dataset[:,-1]\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
            "(768,)\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0xMqOyTs5xt",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp9USczrfu6M",
        "colab_type": "code",
        "outputId": "c79caba3-5107-4c47-917e-02b9ff587d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNO_qrOvT9T4",
        "colab_type": "code",
        "outputId": "d76bd8f5-5ef4-405e-f5f9-da5ffb364948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21016
        }
      },
      "source": [
        "help(Sequential)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Sequential in module keras.engine.sequential:\n",
            "\n",
            "class Sequential(keras.engine.training.Model)\n",
            " |  Linear stack of layers.\n",
            " |  \n",
            " |  # Arguments\n",
            " |      layers: list of layers to add to the model.\n",
            " |  \n",
            " |  # Example\n",
            " |  \n",
            " |  ```python\n",
            " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(500,)))\n",
            " |  \n",
            " |  # Afterwards, we do automatic shape inference:\n",
            " |  model.add(Dense(32))\n",
            " |  \n",
            " |  # This is identical to the following:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_dim=500))\n",
            " |  \n",
            " |  # And to the following:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, batch_input_shape=(None, 500)))\n",
            " |  \n",
            " |  # Note that you can also omit the `input_shape` argument:\n",
            " |  # In that case the model gets built the first time you call `fit` (or other\n",
            " |  # training and evaluation methods).\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.compile(optimizer=optimizer, loss=loss)\n",
            " |  \n",
            " |  # This builds the model for the first time:\n",
            " |  model.fit(x, y, batch_size=32, epochs=10)\n",
            " |  \n",
            " |  # Note that when using this delayed-build pattern\n",
            " |  # (no input shape specified),\n",
            " |  # the model doesn't have any weights until the first call\n",
            " |  # to a training/evaluation method (since it isn't yet built):\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.weights  # returns []\n",
            " |  \n",
            " |  # Whereas if you specify the input shape, the model gets built continuously\n",
            " |  # as you are adding layers:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(500,)))\n",
            " |  model.add(Dense(32))\n",
            " |  model.weights  # returns list of length 4\n",
            " |  \n",
            " |  # When using the delayed-build pattern (no input shape specified), you can\n",
            " |  # choose to manually build your model by calling\n",
            " |  # `build(batch_input_shape)`:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.build((None, 500))\n",
            " |  model.weights  # returns list of length 4\n",
            " |  ```\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Sequential\n",
            " |      keras.engine.training.Model\n",
            " |      keras.engine.network.Network\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, layers=None, name=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  add(self, layer)\n",
            " |      Adds a layer instance on top of the layer stack.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          layer: layer instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          TypeError: If `layer` is not a layer instance.\n",
            " |          ValueError: In case the `layer` argument does not\n",
            " |              know its input shape.\n",
            " |          ValueError: In case the `layer` argument has\n",
            " |              multiple output tensors, or is already connected\n",
            " |              somewhere else (forbidden in `Sequential` models).\n",
            " |  \n",
            " |  build(self, input_shape=None)\n",
            " |      Creates the layer weights.\n",
            " |      \n",
            " |      Must be implemented on all layers that have weights.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Keras tensor (future input to layer)\n",
            " |              or list/tuple of Keras tensors to reference\n",
            " |              for weight shape computations.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  pop(self)\n",
            " |      Removes the last layer in the model.\n",
            " |      \n",
            " |      # Raises\n",
            " |          TypeError: if there are no layers in the model.\n",
            " |  \n",
            " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
            " |      Generate class predictions for the input samples.\n",
            " |      \n",
            " |      The input samples are processed batch by batch.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: input data, as a Numpy array or list of Numpy arrays\n",
            " |              (if the model has multiple inputs).\n",
            " |          batch_size: integer.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns:\n",
            " |          A numpy array of class predictions.\n",
            " |  \n",
            " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
            " |      Generates class probability predictions for the input samples.\n",
            " |      \n",
            " |      The input samples are processed batch by batch.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: input data, as a Numpy array or list of Numpy arrays\n",
            " |              (if the model has multiple inputs).\n",
            " |          batch_size: integer.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy array of probability predictions.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Instantiates a Model from its config (output of `get_config()`).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          config: Model config dictionary.\n",
            " |          custom_objects: Optional dictionary mapping names\n",
            " |              (strings) to custom classes or functions to be\n",
            " |              considered during deserialization.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A model instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of improperly formatted config dict.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  model\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          optimizer: String (name of optimizer) or optimizer instance.\n",
            " |              See [optimizers](/optimizers).\n",
            " |          loss: String (name of objective function) or objective function.\n",
            " |              See [losses](/losses).\n",
            " |              If the model has multiple outputs, you can use a different loss\n",
            " |              on each output by passing a dictionary or a list of losses.\n",
            " |              The loss value that will be minimized by the model\n",
            " |              will then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model\n",
            " |              during training and testing.\n",
            " |              Typically you will use `metrics=['accuracy']`.\n",
            " |              To specify different metrics for different outputs of a\n",
            " |              multi-output model, you could also pass a dictionary,\n",
            " |              such as `metrics={'output_a': 'accuracy'}`.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar\n",
            " |              coefficients (Python floats) to weight the loss contributions\n",
            " |              of different model outputs.\n",
            " |              The loss value that will be minimized by the model\n",
            " |              will then be the *weighted sum* of all individual losses,\n",
            " |              weighted by the `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping\n",
            " |              to the model's outputs. If a tensor, it is expected to map\n",
            " |              output names (strings) to scalar coefficients.\n",
            " |          sample_weight_mode: If you need to do timestep-wise\n",
            " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
            " |              `None` defaults to sample-wise weights (1D).\n",
            " |              If the model has multiple outputs, you can use a different\n",
            " |              `sample_weight_mode` on each output by passing a\n",
            " |              dictionary or a list of modes.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
            " |              by sample_weight or class_weight during training and testing.\n",
            " |          target_tensors: By default, Keras will create placeholders for the\n",
            " |              model's target, which will be fed with the target data during\n",
            " |              training. If instead you would like to use your own\n",
            " |              target tensors (in turn, Keras will not expect external\n",
            " |              Numpy data for these targets at training time), you\n",
            " |              can specify them via the `target_tensors` argument. It can be\n",
            " |              a single tensor (for a single-output model), a list of tensors,\n",
            " |              or a dict mapping output names to target tensors.\n",
            " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
            " |              are passed into `K.function`.\n",
            " |              When using the TensorFlow backend,\n",
            " |              these arguments are passed into `tf.Session.run`.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of test data (if the model has a single input),\n",
            " |              or list of Numpy arrays (if the model has multiple inputs).\n",
            " |              If input layers in the model are named, you can also pass a\n",
            " |              dictionary mapping input names to Numpy arrays.\n",
            " |              `x` can be `None` (default) if feeding from\n",
            " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
            " |          y: Numpy array of target (label) data\n",
            " |              (if the model has a single output),\n",
            " |              or list of Numpy arrays (if the model has multiple outputs).\n",
            " |              If output layers in the model are named, you can also pass a\n",
            " |              dictionary mapping output names to Numpy arrays.\n",
            " |              `y` can be `None` (default) if feeding from\n",
            " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per evaluation step.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |          verbose: 0 or 1. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the test samples, used for weighting the loss function.\n",
            " |              You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
            " |          steps: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring the evaluation round finished.\n",
            " |              Ignored with the default value of `None`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      The generator should return the same kind of data\n",
            " |      as accepted by `test_on_batch`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: Generator yielding tuples (inputs, targets)\n",
            " |              or (inputs, targets, sample_weights)\n",
            " |              or an instance of Sequence (keras.utils.Sequence)\n",
            " |              object in order to avoid duplicate data\n",
            " |              when using multiprocessing.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before stopping.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          max_queue_size: maximum size for the generator queue\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: if True, use process based threading.\n",
            " |              Note that because\n",
            " |              this implementation relies on multiprocessing,\n",
            " |              you should not pass\n",
            " |              non picklable arguments to the generator\n",
            " |              as they can't be passed\n",
            " |              easily to children processes.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields\n",
            " |              data in an invalid format.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
            " |      Trains the model for a given number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of training data (if the model has a single input),\n",
            " |              or list of Numpy arrays (if the model has multiple inputs).\n",
            " |              If input layers in the model are named, you can also pass a\n",
            " |              dictionary mapping input names to Numpy arrays.\n",
            " |              `x` can be `None` (default) if feeding from\n",
            " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
            " |          y: Numpy array of target (label) data\n",
            " |              (if the model has a single output),\n",
            " |              or list of Numpy arrays (if the model has multiple outputs).\n",
            " |              If output layers in the model are named, you can also pass a\n",
            " |              dictionary mapping output names to Numpy arrays.\n",
            " |              `y` can be `None` (default) if feeding from\n",
            " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling.\n",
            " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
            " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch').\n",
            " |              'batch' is a special option for dealing with the\n",
            " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
            " |              Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined.\n",
            " |          validation_steps: Only relevant if `steps_per_epoch`\n",
            " |              is specified. Total number of steps (batches of samples)\n",
            " |              to validate before stopping.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: If the model was never compiled.\n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Trains the model on data generated batch-by-batch by a Python generator\n",
            " |      (or an instance of `Sequence`).\n",
            " |      \n",
            " |      The generator is run in parallel to the model, for efficiency.\n",
            " |      For instance, this allows you to do real-time data augmentation\n",
            " |      on images on CPU in parallel to training your model on GPU.\n",
            " |      \n",
            " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
            " |      and guarantees the single use of every input per epoch when\n",
            " |      using `use_multiprocessing=True`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: A generator or an instance of `Sequence`\n",
            " |              (`keras.utils.Sequence`) object in order to avoid\n",
            " |              duplicate data when using multiprocessing.\n",
            " |              The output of the generator must be either\n",
            " |              - a tuple `(inputs, targets)`\n",
            " |              - a tuple `(inputs, targets, sample_weights)`.\n",
            " |              This tuple (a single output of the generator) makes a single\n",
            " |              batch. Therefore, all arrays in this tuple must have the same\n",
            " |              length (equal to the size of this batch). Different batches may\n",
            " |              have different sizes. For example, the last batch of the epoch\n",
            " |              is commonly smaller than the others, if the size of the dataset\n",
            " |              is not divisible by the batch size.\n",
            " |              The generator is expected to loop over its data\n",
            " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
            " |              batches have been seen by the model.\n",
            " |          steps_per_epoch: Integer.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before declaring one epoch\n",
            " |              finished and starting the next epoch. It should typically\n",
            " |              be equal to the number of samples of your dataset\n",
            " |              divided by the batch size.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire data provided,\n",
            " |              as defined by `steps_per_epoch`.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          validation_data: This can be either\n",
            " |              - a generator or a `Sequence` object for the validation data\n",
            " |              - tuple `(x_val, y_val)`\n",
            " |              - tuple `(x_val, y_val, val_sample_weights)`\n",
            " |              on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data.\n",
            " |          validation_steps: Only relevant if `validation_data`\n",
            " |              is a generator. Total number of steps (batches of samples)\n",
            " |              to yield from `validation_data` generator before stopping\n",
            " |              at the end of every epoch. It should typically\n",
            " |              be equal to the number of samples of your\n",
            " |              validation dataset divided by the batch size.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(validation_data)` as a number of steps.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only). This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples\n",
            " |              from an under-represented class.\n",
            " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process-based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean.\n",
            " |              If `True`, use process-based threading.\n",
            " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
            " |              Note that because this implementation\n",
            " |              relies on multiprocessing,\n",
            " |              you should not pass non-picklable arguments to the generator\n",
            " |              as they can't be passed easily to children processes.\n",
            " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
            " |              the beginning of each epoch. Only used with instances\n",
            " |              of `Sequence` (`keras.utils.Sequence`).\n",
            " |              Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields data in an invalid format.\n",
            " |      \n",
            " |      # Example\n",
            " |      \n",
            " |      ```python\n",
            " |      def generate_arrays_from_file(path):\n",
            " |          while True:\n",
            " |              with open(path) as f:\n",
            " |                  for line in f:\n",
            " |                      # create numpy arrays of input data\n",
            " |                      # and labels, from each line in the file\n",
            " |                      x1, x2, y = process_line(line)\n",
            " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
            " |      \n",
            " |      model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
            " |                          steps_per_epoch=10000, epochs=10)\n",
            " |      ```\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: The input data, as a Numpy array\n",
            " |              (or list of Numpy arrays if the model has multiple inputs).\n",
            " |          batch_size: Integer. If unspecified, it will default to 32.\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      The generator should return the same kind of data as accepted by\n",
            " |      `predict_on_batch`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: Generator yielding batches of input samples\n",
            " |              or an instance of Sequence (keras.utils.Sequence)\n",
            " |              object in order to avoid duplicate data\n",
            " |              when using multiprocessing.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before stopping.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          max_queue_size: Maximum size for the generator queue.\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: If `True`, use process based threading.\n",
            " |              Note that because\n",
            " |              this implementation relies on multiprocessing,\n",
            " |              you should not pass\n",
            " |              non picklable arguments to the generator\n",
            " |              as they can't be passed\n",
            " |              easily to children processes.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields\n",
            " |              data in an invalid format.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Input samples, as a Numpy array.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |  \n",
            " |  test_on_batch(self, x, y, sample_weight=None)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of test data,\n",
            " |              or list of Numpy arrays if the model has multiple inputs.\n",
            " |              If all inputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping input names to Numpy arrays.\n",
            " |          y: Numpy array of target data,\n",
            " |              or list of Numpy arrays if the model has multiple outputs.\n",
            " |              If all outputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping output names to Numpy arrays.\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |              weights to apply to the model's loss for each sample.\n",
            " |              In the case of temporal data, you can pass a 2D array\n",
            " |              with shape (samples, sequence_length),\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              sample_weight_mode=\"temporal\" in compile().\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of training data,\n",
            " |              or list of Numpy arrays if the model has multiple inputs.\n",
            " |              If all inputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping input names to Numpy arrays.\n",
            " |          y: Numpy array of target data,\n",
            " |              or list of Numpy arrays if the model has multiple outputs.\n",
            " |              If all outputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping output names to Numpy arrays.\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |              weights to apply to the model's loss for each sample.\n",
            " |              In the case of temporal data, you can pass a 2D array\n",
            " |              with shape (samples, sequence_length),\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              sample_weight_mode=\"temporal\" in compile().\n",
            " |          class_weight: Optional dictionary mapping\n",
            " |              class indices (integers) to\n",
            " |              a weight (float) to apply to the model's loss for the samples\n",
            " |              from this class during training.\n",
            " |              This can be useful to tell the model to \"pay more attention\" to\n",
            " |              samples from an under-represented class.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.network.Network:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  call(self, inputs, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      A model is callable on non-Keras tensors.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      # Returns\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      Assumes that the layer will be built\n",
            " |      to match that input shape provided.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      \n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)\n",
            " |      Loads all layer weights from a HDF5 save file.\n",
            " |      \n",
            " |      If `by_name` is False (default) weights are loaded\n",
            " |      based on the network's topology, meaning the architecture\n",
            " |      should be the same as when the weights were saved.\n",
            " |      Note that layers that don't have weights are not taken\n",
            " |      into account in the topological ordering, so adding or\n",
            " |      removing layers is fine as long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers\n",
            " |      only if they share the same name. This is useful\n",
            " |      for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: String, path to the weights file to load.\n",
            " |          by_name: Boolean, whether to load weights by name\n",
            " |              or by topological order.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
            " |              where there is a mismatch in the number of weights,\n",
            " |              or a mismatch in the shape of the weight\n",
            " |              (only valid when `by_name`=True).\n",
            " |          reshape: Reshape weights to fit the layer when the correct number\n",
            " |              of weight arrays is present but their shape does not match.\n",
            " |      \n",
            " |      \n",
            " |      # Raises\n",
            " |          ImportError: If h5py is not available.\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  run_internal_graph(self, inputs, masks=None)\n",
            " |      Computes output tensors for new inputs.\n",
            " |      \n",
            " |      # Note:\n",
            " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
            " |          - Can be run on non-Keras tensors.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: List of tensors\n",
            " |          masks: List of masks (tensors or None).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Three lists: output_tensors, output_masks, output_shapes\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
            " |      Saves the model to a single HDF5 file.\n",
            " |      \n",
            " |      The savefile includes:\n",
            " |          - The model architecture, allowing to re-instantiate the model.\n",
            " |          - The model weights.\n",
            " |          - The state of the optimizer, allowing to resume training\n",
            " |              exactly where you left off.\n",
            " |      \n",
            " |      This allows you to save the entirety of the state of a model\n",
            " |      in a single file.\n",
            " |      \n",
            " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
            " |      The model returned by `load_model`\n",
            " |      is a compiled model ready to be used (unless the saved model\n",
            " |      was never compiled in the first place).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: String, path to the file to save the weights to.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |      \n",
            " |      # Example\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True)\n",
            " |      Dumps all layer weights to a HDF5 file.\n",
            " |      \n",
            " |      The weight file has:\n",
            " |          - `layer_names` (attribute), a list of strings\n",
            " |              (ordered names of model layers).\n",
            " |          - For every layer, a `group` named `layer.name`\n",
            " |              - For every such layer group, a group attribute `weight_names`,\n",
            " |                  a list of strings\n",
            " |                  (ordered names of weights tensor of the layer).\n",
            " |              - For every weight in the layer, a dataset\n",
            " |                  storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: String, path to the file to save the weights to.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ImportError: If h5py is not available.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the model.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          weights: A list of Numpy arrays with shapes and types matching\n",
            " |              the output of `model.get_weights()`.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |              It defaults to `print` (prints to stdout).\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A YAML string.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.network.Network:\n",
            " |  \n",
            " |  input_spec\n",
            " |      Gets the model's input specs.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of `InputSpec` instances (one per input to the model)\n",
            " |              or a single instance if the model has only one input.\n",
            " |  \n",
            " |  losses\n",
            " |      Retrieves the model's losses.\n",
            " |      \n",
            " |      Will only include losses that are either\n",
            " |      unconditional, or conditional on inputs to this model\n",
            " |      (e.g. will not include losses that depend on tensors\n",
            " |      that aren't inputs to this model).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of loss tensors.\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  state_updates\n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  updates\n",
            " |      Retrieves the model's updates.\n",
            " |      \n",
            " |      Will only include updates that are either\n",
            " |      unconditional, or conditional on inputs to this model\n",
            " |      (e.g. will not include updates that depend on tensors\n",
            " |      that aren't inputs to this model).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  uses_learning_phase\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, inputs, **kwargs)\n",
            " |      Wrapper around self.call(), for handling internal references.\n",
            " |      \n",
            " |      If a Keras tensor is passed:\n",
            " |          - We call self._add_inbound_node().\n",
            " |          - If necessary, we `build` the layer to match\n",
            " |              the _keras_shape of the input(s).\n",
            " |          - We update the _keras_shape of every input tensor with\n",
            " |              its new shape (obtained via self.compute_output_shape).\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |          - We update the _keras_history of the output tensor(s)\n",
            " |              with the current layer.\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Can be a tensor or list/tuple of tensors.\n",
            " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output of the layer's `call` method.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case the layer is missing shape information\n",
            " |              for its `build` call.\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Adds losses to the layer.\n",
            " |      \n",
            " |      The loss may potentially be conditional on some inputs tensors,\n",
            " |      for instance activity losses are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          losses: loss tensor or list of loss tensors\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the losses as conditional on these inputs.\n",
            " |              If None is passed, the loss is assumed unconditional\n",
            " |              (e.g. L2 weight regularization, which only depends\n",
            " |              on the layer's weights variables, not on any inputs tensors).\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Adds updates to the layer.\n",
            " |      \n",
            " |      The updates may potentially be conditional on some inputs tensors,\n",
            " |      for instance batch norm updates are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          updates: update op or list of update ops\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the updates as conditional on these inputs.\n",
            " |              If None is passed, the updates are assumed unconditional.\n",
            " |  \n",
            " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
            " |      Adds a weight variable to the layer.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, the name for the weight variable.\n",
            " |          shape: The shape tuple of the weight.\n",
            " |          dtype: The dtype of the weight.\n",
            " |          initializer: An Initializer instance (callable).\n",
            " |          regularizer: An optional Regularizer instance.\n",
            " |          trainable: A boolean, whether the weight should\n",
            " |              be trained via backprop or not (assuming\n",
            " |              that the layer itself is also trainable).\n",
            " |          constraint: An optional Constraint instance.\n",
            " |      \n",
            " |      # Returns\n",
            " |          The created weight variable.\n",
            " |  \n",
            " |  assert_input_compatibility(self, inputs)\n",
            " |      Checks compatibility between the layer and provided inputs.\n",
            " |      \n",
            " |      This checks that the tensor(s) `input`\n",
            " |      verify the input assumptions of the layer\n",
            " |      (if any). If not, exceptions are raised.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case of mismatch between\n",
            " |              the provided inputs and the expectations of the layer.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Counts the total number of scalars composing the weights.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An integer count.\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: if the layer isn't yet built\n",
            " |              (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  built\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input shape tuple\n",
            " |          (or list of input shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output tensor or list of output tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one inbound node,\n",
            " |      or if all inbound nodes have the same output shape.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output shape tuple\n",
            " |          (or list of input shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqOHQVchUKys",
        "colab_type": "code",
        "outputId": "ae3a35dd-37fd-428c-806b-f1ed3685ed0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7904
        }
      },
      "source": [
        "help(Dense)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Dense in module keras.layers.core:\n",
            "\n",
            "class Dense(keras.engine.base_layer.Layer)\n",
            " |  Just your regular densely-connected NN layer.\n",
            " |  \n",
            " |  `Dense` implements the operation:\n",
            " |  `output = activation(dot(input, kernel) + bias)`\n",
            " |  where `activation` is the element-wise activation function\n",
            " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
            " |  created by the layer, and `bias` is a bias vector created by the layer\n",
            " |  (only applicable if `use_bias` is `True`).\n",
            " |  \n",
            " |  Note: if the input to the layer has a rank greater than 2, then\n",
            " |  it is flattened prior to the initial dot product with `kernel`.\n",
            " |  \n",
            " |  # Example\n",
            " |  \n",
            " |  ```python\n",
            " |      # as first layer in a sequential model:\n",
            " |      model = Sequential()\n",
            " |      model.add(Dense(32, input_shape=(16,)))\n",
            " |      # now the model will take as input arrays of shape (*, 16)\n",
            " |      # and output arrays of shape (*, 32)\n",
            " |  \n",
            " |      # after the first layer, you don't need to specify\n",
            " |      # the size of the input anymore:\n",
            " |      model.add(Dense(32))\n",
            " |  ```\n",
            " |  \n",
            " |  # Arguments\n",
            " |      units: Positive integer, dimensionality of the output space.\n",
            " |      activation: Activation function to use\n",
            " |          (see [activations](../activations.md)).\n",
            " |          If you don't specify anything, no activation is applied\n",
            " |          (ie. \"linear\" activation: `a(x) = x`).\n",
            " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
            " |      kernel_initializer: Initializer for the `kernel` weights matrix\n",
            " |          (see [initializers](../initializers.md)).\n",
            " |      bias_initializer: Initializer for the bias vector\n",
            " |          (see [initializers](../initializers.md)).\n",
            " |      kernel_regularizer: Regularizer function applied to\n",
            " |          the `kernel` weights matrix\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      bias_regularizer: Regularizer function applied to the bias vector\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      activity_regularizer: Regularizer function applied to\n",
            " |          the output of the layer (its \"activation\").\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      kernel_constraint: Constraint function applied to\n",
            " |          the `kernel` weights matrix\n",
            " |          (see [constraints](../constraints.md)).\n",
            " |      bias_constraint: Constraint function applied to the bias vector\n",
            " |          (see [constraints](../constraints.md)).\n",
            " |  \n",
            " |  # Input shape\n",
            " |      nD tensor with shape: `(batch_size, ..., input_dim)`.\n",
            " |      The most common situation would be\n",
            " |      a 2D input with shape `(batch_size, input_dim)`.\n",
            " |  \n",
            " |  # Output shape\n",
            " |      nD tensor with shape: `(batch_size, ..., units)`.\n",
            " |      For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
            " |      the output would have shape `(batch_size, units)`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Dense\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the layer weights.\n",
            " |      \n",
            " |      Must be implemented on all layers that have weights.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Keras tensor (future input to layer)\n",
            " |              or list/tuple of Keras tensors to reference\n",
            " |              for weight shape computations.\n",
            " |  \n",
            " |  call(self, inputs)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Input tensor, or list/tuple of input tensors.\n",
            " |          **kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      Assumes that the layer will be built\n",
            " |      to match that input shape provided.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, inputs, **kwargs)\n",
            " |      Wrapper around self.call(), for handling internal references.\n",
            " |      \n",
            " |      If a Keras tensor is passed:\n",
            " |          - We call self._add_inbound_node().\n",
            " |          - If necessary, we `build` the layer to match\n",
            " |              the _keras_shape of the input(s).\n",
            " |          - We update the _keras_shape of every input tensor with\n",
            " |              its new shape (obtained via self.compute_output_shape).\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |          - We update the _keras_history of the output tensor(s)\n",
            " |              with the current layer.\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Can be a tensor or list/tuple of tensors.\n",
            " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output of the layer's `call` method.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case the layer is missing shape information\n",
            " |              for its `build` call.\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Adds losses to the layer.\n",
            " |      \n",
            " |      The loss may potentially be conditional on some inputs tensors,\n",
            " |      for instance activity losses are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          losses: loss tensor or list of loss tensors\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the losses as conditional on these inputs.\n",
            " |              If None is passed, the loss is assumed unconditional\n",
            " |              (e.g. L2 weight regularization, which only depends\n",
            " |              on the layer's weights variables, not on any inputs tensors).\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Adds updates to the layer.\n",
            " |      \n",
            " |      The updates may potentially be conditional on some inputs tensors,\n",
            " |      for instance batch norm updates are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          updates: update op or list of update ops\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the updates as conditional on these inputs.\n",
            " |              If None is passed, the updates are assumed unconditional.\n",
            " |  \n",
            " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
            " |      Adds a weight variable to the layer.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, the name for the weight variable.\n",
            " |          shape: The shape tuple of the weight.\n",
            " |          dtype: The dtype of the weight.\n",
            " |          initializer: An Initializer instance (callable).\n",
            " |          regularizer: An optional Regularizer instance.\n",
            " |          trainable: A boolean, whether the weight should\n",
            " |              be trained via backprop or not (assuming\n",
            " |              that the layer itself is also trainable).\n",
            " |          constraint: An optional Constraint instance.\n",
            " |      \n",
            " |      # Returns\n",
            " |          The created weight variable.\n",
            " |  \n",
            " |  assert_input_compatibility(self, inputs)\n",
            " |      Checks compatibility between the layer and provided inputs.\n",
            " |      \n",
            " |      This checks that the tensor(s) `input`\n",
            " |      verify the input assumptions of the layer\n",
            " |      (if any). If not, exceptions are raised.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case of mismatch between\n",
            " |              the provided inputs and the expectations of the layer.\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      # Returns\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Counts the total number of scalars composing the weights.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An integer count.\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: if the layer isn't yet built\n",
            " |              (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  built\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input shape tuple\n",
            " |          (or list of input shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  losses\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output tensor or list of output tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one inbound node,\n",
            " |      or if all inbound nodes have the same output shape.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output shape tuple\n",
            " |          (or list of input shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAzHLg27thoN",
        "colab_type": "text"
      },
      "source": [
        "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSNsL49Xp6KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/getting-started/sequential-model-guide/\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCYX6QzJtvpG",
        "colab_type": "text"
      },
      "source": [
        "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
        "\n",
        " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
        " \n",
        " The first argument is how many neurons we want to have in that layer. To create a perceptron model we will just set it to 1. We will tell it that there will be 8 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNzOLidxtvFa",
        "colab_type": "code",
        "outputId": "4bbe16e6-af62-437a-fb85-567acc5f22f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "model.add(Dense(1, input_dim=8, activation=\"sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnI3jwKMtBL2",
        "colab_type": "text"
      },
      "source": [
        "### Compile Model\n",
        "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3768Z69VLoU",
        "colab_type": "code",
        "outputId": "929d266d-4525-4150-a72d-4e104cdd1fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "source": [
        "help(model.compile)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method compile in module keras.engine.training:\n",
            "\n",
            "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
            "    Configures the model for training.\n",
            "    \n",
            "    # Arguments\n",
            "        optimizer: String (name of optimizer) or optimizer instance.\n",
            "            See [optimizers](/optimizers).\n",
            "        loss: String (name of objective function) or objective function.\n",
            "            See [losses](/losses).\n",
            "            If the model has multiple outputs, you can use a different loss\n",
            "            on each output by passing a dictionary or a list of losses.\n",
            "            The loss value that will be minimized by the model\n",
            "            will then be the sum of all individual losses.\n",
            "        metrics: List of metrics to be evaluated by the model\n",
            "            during training and testing.\n",
            "            Typically you will use `metrics=['accuracy']`.\n",
            "            To specify different metrics for different outputs of a\n",
            "            multi-output model, you could also pass a dictionary,\n",
            "            such as `metrics={'output_a': 'accuracy'}`.\n",
            "        loss_weights: Optional list or dictionary specifying scalar\n",
            "            coefficients (Python floats) to weight the loss contributions\n",
            "            of different model outputs.\n",
            "            The loss value that will be minimized by the model\n",
            "            will then be the *weighted sum* of all individual losses,\n",
            "            weighted by the `loss_weights` coefficients.\n",
            "            If a list, it is expected to have a 1:1 mapping\n",
            "            to the model's outputs. If a tensor, it is expected to map\n",
            "            output names (strings) to scalar coefficients.\n",
            "        sample_weight_mode: If you need to do timestep-wise\n",
            "            sample weighting (2D weights), set this to `\"temporal\"`.\n",
            "            `None` defaults to sample-wise weights (1D).\n",
            "            If the model has multiple outputs, you can use a different\n",
            "            `sample_weight_mode` on each output by passing a\n",
            "            dictionary or a list of modes.\n",
            "        weighted_metrics: List of metrics to be evaluated and weighted\n",
            "            by sample_weight or class_weight during training and testing.\n",
            "        target_tensors: By default, Keras will create placeholders for the\n",
            "            model's target, which will be fed with the target data during\n",
            "            training. If instead you would like to use your own\n",
            "            target tensors (in turn, Keras will not expect external\n",
            "            Numpy data for these targets at training time), you\n",
            "            can specify them via the `target_tensors` argument. It can be\n",
            "            a single tensor (for a single-output model), a list of tensors,\n",
            "            or a dict mapping output names to target tensors.\n",
            "        **kwargs: When using the Theano/CNTK backends, these arguments\n",
            "            are passed into `K.function`.\n",
            "            When using the TensorFlow backend,\n",
            "            these arguments are passed into `tf.Session.run`.\n",
            "    \n",
            "    # Raises\n",
            "        ValueError: In case of invalid arguments for\n",
            "            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6xwYaqurRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EWAVbIRVv8c",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dW8SZ2Ls9SX",
        "colab_type": "text"
      },
      "source": [
        "### Fit Model\n",
        "\n",
        "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJxdmX_-u5MJ",
        "colab_type": "code",
        "outputId": "f239efe0-0c37-4281-ba1e-46c56c909368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5316
        }
      },
      "source": [
        "model.fit(X, Y, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/150\n",
            "768/768 [==============================] - 1s 838us/step - loss: 9.9188 - acc: 0.3581\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.8753 - acc: 0.3607\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.8506 - acc: 0.3620\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 9.7781 - acc: 0.3646\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 9.5718 - acc: 0.3672\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 9.3616 - acc: 0.3802\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 9.3026 - acc: 0.3815\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 9.2861 - acc: 0.3880\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 9.2871 - acc: 0.3880\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 9.2802 - acc: 0.3880\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 9.2820 - acc: 0.3893\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 9.2719 - acc: 0.3919\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 9.2606 - acc: 0.3932\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.2576 - acc: 0.3958\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 9.2354 - acc: 0.3945\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 9.2230 - acc: 0.3945\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 9.2092 - acc: 0.3984\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.1821 - acc: 0.4010\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.1348 - acc: 0.4010\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 9.0754 - acc: 0.4023\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 8.9633 - acc: 0.4089\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 8.7566 - acc: 0.4219\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 8.5197 - acc: 0.4336\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 8.2433 - acc: 0.4531\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 8.0534 - acc: 0.4648\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.8589 - acc: 0.4805\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 7.7318 - acc: 0.4935\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 7.6671 - acc: 0.5000\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 7.5965 - acc: 0.5091\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 7.5433 - acc: 0.5143\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.4909 - acc: 0.5169\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.4368 - acc: 0.5143\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.3942 - acc: 0.5182\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.3699 - acc: 0.5143\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 7.3375 - acc: 0.5117\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 7.2858 - acc: 0.5104\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 7.0616 - acc: 0.5130\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 5.9675 - acc: 0.5247\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 4.9189 - acc: 0.5820\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.8335 - acc: 0.5911\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 4.7852 - acc: 0.5885\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 4.7354 - acc: 0.5885\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 32us/step - loss: 4.6787 - acc: 0.5898\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 4.6185 - acc: 0.5924\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 4.5676 - acc: 0.5898\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 4.5048 - acc: 0.5833\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 4.4134 - acc: 0.5885\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 4.3273 - acc: 0.5911\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 4.2433 - acc: 0.5911\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 4.1450 - acc: 0.5846\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.0424 - acc: 0.5859\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 3.9311 - acc: 0.5833\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.8278 - acc: 0.5846\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.7196 - acc: 0.5794\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.6179 - acc: 0.5977\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.5195 - acc: 0.5846\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.4414 - acc: 0.5924\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.3568 - acc: 0.6003\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.2864 - acc: 0.5990\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.2298 - acc: 0.6094\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.1842 - acc: 0.6159\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.1496 - acc: 0.6263\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.1209 - acc: 0.6172\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0938 - acc: 0.6302\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0866 - acc: 0.6341\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.0680 - acc: 0.6367\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0602 - acc: 0.6432\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0531 - acc: 0.6471\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0476 - acc: 0.6576\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0418 - acc: 0.6615\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0405 - acc: 0.6615\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0408 - acc: 0.6602\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.0330 - acc: 0.6719\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0353 - acc: 0.6745\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0333 - acc: 0.6732\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0329 - acc: 0.6784\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 33us/step - loss: 3.0330 - acc: 0.6784\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 33us/step - loss: 3.0338 - acc: 0.6628\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0249 - acc: 0.6654\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0242 - acc: 0.6745\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0254 - acc: 0.6771\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0234 - acc: 0.6732\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0216 - acc: 0.6797\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0255 - acc: 0.6745\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 32us/step - loss: 3.0268 - acc: 0.6758\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0194 - acc: 0.6823\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0218 - acc: 0.6732\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0204 - acc: 0.6888\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0189 - acc: 0.6758\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 3.0176 - acc: 0.6862\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 3.0197 - acc: 0.6823\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 3.0202 - acc: 0.6849\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0184 - acc: 0.6797\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0176 - acc: 0.6836\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0167 - acc: 0.6836\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 32us/step - loss: 3.0213 - acc: 0.6849\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 33us/step - loss: 3.0185 - acc: 0.6875\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0146 - acc: 0.6849\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0157 - acc: 0.6810\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0157 - acc: 0.6875\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0171 - acc: 0.6797\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0151 - acc: 0.6849\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 3.0146 - acc: 0.6836\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0136 - acc: 0.6810\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0145 - acc: 0.6888\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0135 - acc: 0.6862\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0131 - acc: 0.6862\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0196 - acc: 0.6784\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.0151 - acc: 0.6914\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0140 - acc: 0.6823\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0156 - acc: 0.6888\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0162 - acc: 0.6836\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0159 - acc: 0.6758\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0136 - acc: 0.6888\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0155 - acc: 0.6940\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 32us/step - loss: 3.0134 - acc: 0.6927\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0164 - acc: 0.6823\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0303 - acc: 0.6589\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0197 - acc: 0.6862\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.0129 - acc: 0.6862\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0103 - acc: 0.6901\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0157 - acc: 0.6849\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0137 - acc: 0.6927\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0102 - acc: 0.6823\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0121 - acc: 0.6901\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 50us/step - loss: 3.0115 - acc: 0.6927\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0141 - acc: 0.6836\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0121 - acc: 0.6810\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0106 - acc: 0.6914\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0111 - acc: 0.6836\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0109 - acc: 0.6875\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0142 - acc: 0.6810\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.0095 - acc: 0.6953\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0106 - acc: 0.6836\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0114 - acc: 0.6940\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0110 - acc: 0.6849\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 3.0099 - acc: 0.6914\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 32us/step - loss: 3.0120 - acc: 0.6784\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 3.0104 - acc: 0.6914\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0094 - acc: 0.6953\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0111 - acc: 0.6875\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0074 - acc: 0.6901\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0099 - acc: 0.6836\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 33us/step - loss: 3.0069 - acc: 0.6953\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0118 - acc: 0.6862\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.0139 - acc: 0.6875\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 33us/step - loss: 3.0143 - acc: 0.6823\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 3.0110 - acc: 0.6888\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0068 - acc: 0.6901\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.0108 - acc: 0.6836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2803f6d940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W9YLGBCWdKZ",
        "colab_type": "code",
        "outputId": "f32bb512-4702-42c3-f5d4-c4053985c3dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwsgYhmiXMpD",
        "colab_type": "code",
        "outputId": "50d1346b-e5df-4c68-d49c-12bd7c98ed4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(Y) / len(Y)  # Predicting \"never diabetes\" is about 65% accurate!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3489583333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CiXg2NQXwi6",
        "colab_type": "code",
        "outputId": "447afab1-db33-4b5a-d510-eb141a4261a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2969
        }
      },
      "source": [
        "dir(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_add_inbound_node',\n",
              " '_base_init',\n",
              " '_build_input_shape',\n",
              " '_built',\n",
              " '_check_trainable_weights_consistency',\n",
              " '_collected_trainable_weights',\n",
              " '_compute_previous_mask',\n",
              " '_expects_training_arg',\n",
              " '_feed_input_names',\n",
              " '_feed_input_shapes',\n",
              " '_feed_inputs',\n",
              " '_feed_loss_fns',\n",
              " '_feed_output_names',\n",
              " '_feed_output_shapes',\n",
              " '_feed_outputs',\n",
              " '_feed_sample_weight_modes',\n",
              " '_feed_sample_weights',\n",
              " '_feed_targets',\n",
              " '_function_kwargs',\n",
              " '_get_node_attribute_at_index',\n",
              " '_inbound_nodes',\n",
              " '_init_graph_network',\n",
              " '_init_subclassed_network',\n",
              " '_initial_weights',\n",
              " '_input_coordinates',\n",
              " '_input_layers',\n",
              " '_is_compiled',\n",
              " '_is_graph_network',\n",
              " '_layers',\n",
              " '_layers_by_depth',\n",
              " '_losses',\n",
              " '_make_predict_function',\n",
              " '_make_test_function',\n",
              " '_make_train_function',\n",
              " '_network_nodes',\n",
              " '_node_key',\n",
              " '_nodes_by_depth',\n",
              " '_outbound_nodes',\n",
              " '_output_coordinates',\n",
              " '_output_layers',\n",
              " '_output_mask_cache',\n",
              " '_output_shape_cache',\n",
              " '_output_tensor_cache',\n",
              " '_per_input_losses',\n",
              " '_per_input_updates',\n",
              " '_set_inputs',\n",
              " '_standardize_user_data',\n",
              " '_updated_config',\n",
              " '_updates',\n",
              " '_uses_dynamic_learning_phase',\n",
              " '_uses_inputs_arg',\n",
              " 'add',\n",
              " 'add_loss',\n",
              " 'add_update',\n",
              " 'add_weight',\n",
              " 'assert_input_compatibility',\n",
              " 'build',\n",
              " 'built',\n",
              " 'call',\n",
              " 'compile',\n",
              " 'compute_mask',\n",
              " 'compute_output_shape',\n",
              " 'count_params',\n",
              " 'evaluate',\n",
              " 'evaluate_generator',\n",
              " 'fit',\n",
              " 'fit_generator',\n",
              " 'from_config',\n",
              " 'get_config',\n",
              " 'get_input_at',\n",
              " 'get_input_mask_at',\n",
              " 'get_input_shape_at',\n",
              " 'get_layer',\n",
              " 'get_losses_for',\n",
              " 'get_output_at',\n",
              " 'get_output_mask_at',\n",
              " 'get_output_shape_at',\n",
              " 'get_updates_for',\n",
              " 'get_weights',\n",
              " 'history',\n",
              " 'input',\n",
              " 'input_mask',\n",
              " 'input_names',\n",
              " 'input_shape',\n",
              " 'input_spec',\n",
              " 'inputs',\n",
              " 'layers',\n",
              " 'load_weights',\n",
              " 'loss',\n",
              " 'loss_functions',\n",
              " 'loss_weights',\n",
              " 'losses',\n",
              " 'metrics',\n",
              " 'metrics_names',\n",
              " 'metrics_tensors',\n",
              " 'metrics_updates',\n",
              " 'model',\n",
              " 'name',\n",
              " 'non_trainable_weights',\n",
              " 'optimizer',\n",
              " 'output',\n",
              " 'output_mask',\n",
              " 'output_names',\n",
              " 'output_shape',\n",
              " 'outputs',\n",
              " 'pop',\n",
              " 'predict',\n",
              " 'predict_classes',\n",
              " 'predict_function',\n",
              " 'predict_generator',\n",
              " 'predict_on_batch',\n",
              " 'predict_proba',\n",
              " 'reset_states',\n",
              " 'run_internal_graph',\n",
              " 'sample_weight_mode',\n",
              " 'sample_weight_modes',\n",
              " 'sample_weights',\n",
              " 'save',\n",
              " 'save_weights',\n",
              " 'set_weights',\n",
              " 'state_updates',\n",
              " 'stateful',\n",
              " 'stateful_metric_functions',\n",
              " 'stateful_metric_names',\n",
              " 'stop_training',\n",
              " 'summary',\n",
              " 'supports_masking',\n",
              " 'targets',\n",
              " 'test_function',\n",
              " 'test_on_batch',\n",
              " 'to_json',\n",
              " 'to_yaml',\n",
              " 'total_loss',\n",
              " 'train_function',\n",
              " 'train_on_batch',\n",
              " 'trainable',\n",
              " 'trainable_weights',\n",
              " 'updates',\n",
              " 'uses_learning_phase',\n",
              " 'weighted_metrics',\n",
              " 'weights']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSybwWEJtGFm",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34wh8z9MvFMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIJoRBxHy27n",
        "colab_type": "text"
      },
      "source": [
        "# Keras Perceptron Model in 4 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQxyONqKvFxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=8, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, Y, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1wfKUxszPKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHYB7k9q3O8T",
        "colab_type": "text"
      },
      "source": [
        "### Why are we getting such different results if we re-run the model?\n",
        "\n",
        "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueDVpctAzvy8",
        "colab_type": "text"
      },
      "source": [
        "# What architecture should we try?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W2Sc7-LzQo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Tell me your amazing ideas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcjMuxtn6wIQ",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "What is an activation function and how does it work?\n",
        "\n",
        "- Takes in a weighted sum of inputs + a bias from the previous layer and outputs an \"activation\" value.\n",
        "- Based its inputs the neuron decides how 'activated' it should be. This can be thought of as the neuron deciding how strongly to fire. You can also think of it as if the neuron is deciding how much of the signal that it has received to pass onto the next layer. \n",
        "- Our choice of activation function does not only affect signal that is passed forward but also affects the backpropagation algorithm. It affects how we update weights in reverse order since activated weight/input sums become the inputs of the next layer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_b0u8Ch60bA",
        "colab_type": "text"
      },
      "source": [
        "## Step Function\n",
        "\n",
        "![Heaviside Step Function](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/325px-Dirac_distribution_CDF.svg.png)\n",
        "\n",
        "All or nothing, a little extreme, which is fine, but makes updating weights through backpropagation impossible. Why? remember that during backpropagation we use derivatives in order to determine how much to update or not update weights. What is the derivative of the step function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKR0YhIVEnXZ",
        "colab_type": "text"
      },
      "source": [
        "## Linear Function\n",
        "\n",
        "![Linear Function](http://www.roconnell.net/Parent%20function/linear.gif)\n",
        "\n",
        "The linear function takes the opposite tact from the step function and passes the signal onto the next layer by a constant factor. There are problems with this but the biggest problems again lie in backpropagation. The derivative of any linear function is a horizontal line which would indicate that we should update all weights by a constant amount every time -which on balance wouldn't change the behavior of our network. Linear functions are typically only used for very simple tasks where interpretability is important, but if interpretability is your highest priority, you probably shouldn't be using neural networks in the first place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFurIVL6EkQ8",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid Function\n",
        "\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)\n",
        "\n",
        "The sigmoid function works great as an activation function! it's continuously differentiable, its derivative doesn't have a constant slope, and having the higher slope in the middle pushes y value predictions towards extremes which is particularly useful for binary classification problems. I mean, this is why we use it as the squishifier in logistic regression as well. It constrains output, but over repeated epochs pushes predictions towards a strong binary prediction. \n",
        "\n",
        "What's the biggest problem with the sigmoid function? The fact that its slope gets pretty flat so quickly after its departure from zero. This means that updating weights based on its gradient really diminishes the size of our weight updates as our model gets more confident about its classifications. This is why even after so many iterations with our test score example we couldn't reach the levels of fit that our gradient descent based model could reach in just a few epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm6p1HWbEhYi",
        "colab_type": "text"
      },
      "source": [
        "## Tanh Function\n",
        "\n",
        "![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        "\n",
        "What if the sigmoid function didn't get so flat quite as soon when moving away from zero and was a little bit steeper in the middle? That's basically the Tanh function. The Tanh function can actually be created by scaling the sigmoid function by 2 in the y dimension and subtracting 1 from all values. It has basically the same properties as the sigmoid, still struggles from diminishingly flat gradients as we move away from 0, but its derivative is higher around 0 causing weights to move to the extremes a little faster. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOn_L6gEcz1",
        "colab_type": "text"
      },
      "source": [
        "## ReLU Function\n",
        "\n",
        "![ReLU Function](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png)\n",
        "\n",
        "ReLU stands for Rectified Linear Units it is by far the most commonly used activation function in modern neural networks. It doesn't activate neurons that are being passed a negative signal and passes on positive signals. Think about why this might be useful. Remember how a lot of our initial weights got set to negative numbers by chance? This would have dealt with those negative weights a lot faster than the sigmoid function updating. What does the derivative of this function look like? It looks like the step function! This means that not all neurons are activated. With sigmoid basically all of our neurons are passing some amount of signal even if it's small making it hard for the network to differentiate important and less important connections. ReLU turns off a portion of our less important neurons which decreases computational load, but also helps the network learn what the most important connections are faster. \n",
        "\n",
        "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWdvWOBIETwk",
        "colab_type": "text"
      },
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "![Leaky ReLU](https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png)\n",
        "\n",
        "Leaky ReLU accomplishes exactly that! it avoids having a gradient of 0 on the left side of its derivative function. This means that even \"dead\" neurons have a chance of being revived over enough iterations. In some specifications the slope of the leaky left-hand side can also be experimented with as a hyperparameter of the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcAxkNFREMFb",
        "colab_type": "text"
      },
      "source": [
        "## Softmax Function\n",
        "\n",
        "![Softmax Function](https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png)\n",
        "\n",
        "Like the sigmoid function but more useful for multi-class classification problems. The softmax function can take any set of inputs and translate them into probabilities that sum up to 1. This means that we can throw any list of outputs at it and it will translate them into probabilities, this is extremely useful for multi-class classification problems. Like MNIST for example..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-XRRXKHs34",
        "colab_type": "text"
      },
      "source": [
        "## Major takeaways\n",
        "\n",
        "- ReLU is generally better at obtaining the optimal model fit.\n",
        "- Sigmoid and its derivatives are usually better at classification problems.\n",
        "- Softmax for multi-class classification problems. \n",
        "\n",
        "You'll typically see ReLU used for all initial layers and then the final layer being sigmoid or softmax for classification problems. But you can experiment and tune these selections as hyperparameters as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuoXZCCKCI7",
        "colab_type": "text"
      },
      "source": [
        "## MNIST with Keras \n",
        "\n",
        "### This will be a good chance to bring up dropout regularization. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmJ_5azs04pU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Let's do it!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKmx8153w9Ci",
        "colab_type": "text"
      },
      "source": [
        "## What if we use dropout techniques to prevent overfitting? How does that affect our model?\n",
        "\n",
        "![Regularization](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWDopERJ16yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Try it with dropout"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}